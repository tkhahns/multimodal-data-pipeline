Categorization,Type,Feature,Feature Columns in Output,Note for Feature(s) to be Extracted,Name of the Model,Website,General Category
Audio Feature,Audio,Audio volume,oc_audvol,,OpenCV,-,-
Audio Feature,Audio,Change in audio volume,oc_audvol_diff,Difference b/t frames,OpenCV,-,-
Audio Feature,Audio,Average audio pitch,oc_audpit,,OpenCV,-,-
Audio Feature,Audio,Change in audio pitch,oc_audpit_diff,Difference b/t frames,OpenCV,-,-
Emotional Recognition,Audio,Speech emotion/emotional speech classification,"ser_neutral, ser_calm, ser_happy, ser_sad, ser_angry, ser_fear, ser_disgust, ser_ps, ser_boredom",ser_ps = (pleasant surprise),Speech Emotion Recognition,https://github.com/x4nth055/emotion-recognition-using-speech,Speech
Audio Sepation,Audio,Speech Separation,"separated_source_1, separated_source_2, ...","Each output is a time-series audio waveform (NumPy array or tensor), corresponding to one separated speaker. The number of outputs matches the number of speakers in the input mixture. When saving to disk, these are typically named source1hat.wav, source2hat.wav, etc. See: HuggingFace SepFormer Model Card (https://huggingface.co/speechbrain/sepformer-libri3mix)",SepFormer,https://github.com/speechbrain/speechbrain,Speech
Audio transcript,Audio,Time-Accurate Speech Transcription,"WhX_highlight_diarize__speaker1_word_1 ... WhX_highlight_diarize_speaker1_word_2....
WhX_highlight_diarize__speaker2_word_1 ... WhX_highlight_diarize_speaker2_word_2....",Output as the word; Marks words whose timestamps or speaker labels were adjusted during diarization so you can spot low-confidence or boundary words.,WhisperX: Time-Accurate Speech Transcription of Long-Form Audio,https://github.com/m-bain/whisperX,Speech
Audio transcript,Audio,Speech-to-Text,"transcription, hidden_states (optional)","Text transcription of input audio (ASR). Output is a string (or list of strings for batch processing). Frame-level feature vectors (embeddings) from intermediate layers. Shape: [time_steps, feature_dim]. Used for downstream tasks. Multilingual over a variety of languages",XLSR,https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec,Speech
Audio transcript,Audio,Speech-to-Text,"text, score (optional), alignment (optional)","text: predicted transcription or translation (string); score: model confidence (log probability, float, higher is better); alignment: mapping of output tokens to input audio frames or source tokens ( requires enabling)",Speech-to-Text (S2T) Modeling,https://github.com/facebookresearch/fairseq/tree/main/examples/speech_to_text,Speech
Audio feature,Audio,"Spectral Features, Pitch, Rhythm","lbrs_spectral_centroid, lbrs_spectral_bandwidth, lbrs_spectral_flatness, lbrs_spectral_rolloff, lbrs_zero_crossing_rate, lbrs_rmse, lbrs_tempo, lbrs_spectral_flatness_singlevalue, lbrs_spectral_contrast_singlevalue, lbrs_rmse_singlevalue, lbrs_tempo_singlevalue, lbrs_zero_crossing_rate_singlevalue",*_singlevalue is one single value across time points (frame/sec),Librosa,https://github.com/librosa/librosa,Speech features
Audio Feature + Audio Sepation + analysis from different levels,Audio,Speech feature extraction,"(1) Low-level descriptors (LDD with Time Frame): osm_pcm_RMSenergy_sma,osm_loudness_sma,osm_spectralFlux_sma,osm_spectralRollOff25_sma,osm_spectralRollOff75_sma,osm_spectralCentroid_sma,osm_spectralEntropy_sma,osm_spectralSlope_sma,osm_spectralDecrease_sma,osm_mfcc1_sma,osm_mfcc2_sma,osm_mfcc3_sma,osm_mfcc4_sma,osm_mfcc5_sma,osm_mfcc6_sma,osm_mfcc7_sma,osm_mfcc8_sma,osm_mfcc9_sma,osm_mfcc10_sma,osm_mfcc11_sma,osm_mfcc12_sma,osm_F0final_sma,osm_voicingProb_sma,osm_jitterLocal_sma,osm_shimmerLocal_sma,osm_lsf1,osm_lsf2,osm_lsf3,osm_lsf4,osm_lsf5,osm_lsf6,osm_lsf7,osm_lsf8,osm_zcr_sma,osm_psychoacousticHarmonicity_sma,osm_psychoacousticSharpness_sma. 

(2) Functionals: Single-value summaries: osm_mean,osm_stddev,osm_skewness,osm_kurtosis,osm_percentile1.0,osm_percentile5.0,osm_percentile25.0,osm_percentile50.0,osm_percentile75.0,osm_percentile95.0,osm_percentile99.0,osm_min,osm_max,osm_minPos,osm_maxPos,osm_range,osm_quartile1,osm_quartile3,osm_interquartileRange,osm_linregc1,osm_linregc2,osm_linregerr
","Two level of output (1) Low-level descriptors (LDD): Low-Level Descriptors (LLDs) in openSMILE cover seven main categories: Energy & Loudness, including pcm_RMSenergy_sma (short-time RMS energy) and loudness_sma (perceptual loudness estimate); Zero-Crossing & Voicing, including pcm_zcr_sma (zero-crossing rate) and voicingProb_sma (voiced/unvoiced probability); Spectral Features, including spectralFlux_sma, spectralRollOff25.0_sma, spectralRollOff50.0_sma, spectralRollOff80.0_sma, spectralCentroid_sma, spectralEntropy_sma, spectralSlope_sma, spectralDecrease_sma; Cepstral Coefficients (MFCCs), including mfcc1_sma through mfcc12_sma; Pitch & Voice Quality, including F0final_sma (fundamental frequency), jitterLocal_sma, jitterDDP_sma (pitch perturbation), shimmerLocal_sma (amplitude perturbation), logHNR_sma (harmonics-to-noise ratio); Linear Predictive/LSP, including lspFreq1_sma through lspFreq8_sma; and Psychoacoustic & Harmonicity, including psychoacousticHarmonicity_sma, psychoacousticSharpness_sma, alphaRatio_sma, hammarbergIndex_sma, slopeV0_sma, slopeV1_sma. (2) Functionals: Single-value summaries only appear when you switch to the functionals level (e.g. reader.dmLevel=fun or using a ComParE/AVEC config without -lldcsvoutput), where each LLD is reduced to one or more statistical measures (mean, percentiles, regression coefficients, etc.) over the entire file.",openSMILE,https://github.com/audeering/opensmile,Speech/Music
,Audio,Sentiment Analysis,"arvs_batch_size, arvs_n_out, arvs_d_out","The Heinsen routing module itself does not write any files — it just returns a PyTorch tensor. The only “output variable” is whatever you assign that tensor to (by convention x_out), and its contents are: A batch of output capsules (vectors), one capsule per “output position”",AnAlgorithm for Routing Vectors in Sequences,https://github.com/glassroom/heinsen_routing,Text
,Audio,Complex characteristics of word use and how these uses vary across linguistic contexts,contextualized embeddings (optional),"For each input token, a context-dependent embedding vector. Shape: [num_tokens, embedding_dim] (typically 1024). Optionally, embeddings from multiple layers can be extracted. Used as input features for downstream NLP tasks. See ELMo paper - https://aclanthology.org/N18-1202/ and AllenNLP ELMo docs. - https://github.com/allenai/allennlp-models",Deep contextualized word representations,https://paperswithcode.com/paper/deep-contextualized-word-representations,Text
Disentangled attention mechanism + Enhanced mark decoder,Audio,Disentangled Attention Mechanism & Enhanced Mask Decoder,"DEB_SQuAD 1.1_F1/EM, DEB_SQuAD 2.0_F1/EM, DEB_MNLI-m/mm_Acc, DEB_SST-2_Acc, DEB_QNLI_Acc, DEB_CoLA_MCC, DEB_RTE_Acc, DEB_MRPC_Acc/F1, DEB_QQP_Acc/F1, DEB_STS-B_P/S","DEB_* variables are performance summaries (single numbers) computed after DeBERTa produces either token-level or sequence-level outputs. 
DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.",DEBERTA,https://github.com/microsoft/DeBERTa,Text
Test the sentence consistency,Audio,contrastive learning framework,"CSE_STS12, CSE_STS13, CSE_STS14, CSE_STS15, CSE_STS16, CSE_STSBenchmark, CSE_SICKRelatedness, CSE_Avg",Might be not the time_frame format.,SimCSE: Simple Contrastive Learning of Sentence Embeddings,https://github.com/princeton-nlp/SimCSE,Text
New edition of BERT,Audio,Language representation,"alb_mnli, alb_qnli, alb_qqp, alb_rte, alb_sst, alb_mrpc, alb_cola, alb_sts, alb_squad1.1 dev, alb_squad2.0 dev, alb_squad2.0 test, alb_race test (middle/high)",single value,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,https://github.com/google-research/ALBERT,Text
BERT for sentence,Audio,compute dense vector representations for sentences + compute dense vector representations for paragraphs,"(1) embedding models
BERT_tensor = [correlational matrix]

(2) Reranker models
BERT_score = [8.607139 5.506266 6.352977]
","(1) BERT_tensor is a kind of correlational matrix

(2) Reranker models example: 
passages = [
""Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers."",
""Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union."",
""In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs."",
]

# 2a. predict scores pairs of texts
scores = model.predict([(query, passage) for passage in passages])
print(scores)
# => [8.607139 5.506266 6.352977]

# 2b. Rank a list of passages for a query
ranks = model.rank(query, passages, return_documents=True)

print(""Query:"", query)
for rank in ranks:
print(f""- #{rank['corpus_id']} ({rank['score']:.2f}): {rank['text']}"")
""""""
Query: How many people live in Berlin?
- #0 (8.61): Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.
- #2 (6.35): In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.
- #1 (5.51): Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.
""""""",Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,https://github.com/UKPLab/sentence-transformers,Text
text classfication and similarity test + cluster/group,Audio,text classification + semantic similarity + semantic cluster,"An example of the result: 
Embedding for: The quick brown fox jumps over the lazy dog.
[-0.016987282782793045, -0.008949815295636654, -0.0070627182722091675, ...]

The output can be USE_embed_sentence1, USE_embed_sentence2, USE_embed_sentence3 ... (It depends on how many sentences we use.
)","The Universal Sentence Encoder (USE) generates for each input text a fixed-length embedding vector—most commonly 512 dimensions—regardless of the text’s length. These embeddings are single, static vectors per input (sentence, paragraph, etc.), not sequences of time-framed outputs.",Universal Sentence Encoder,https://tfhub.dev/google/universal-sentence-encoder/1,Text
high-quality time-stretching of audio files without altering their pitch,Audio,"(1) High-quality time-stretching of WAV/MP3 files without changing their pitch; 
(2) Time-stretch silence separately","AS_ratio, AS_gap_ratio, AS_lower_freq, AS_upper_freq, AS_buffer_ms, AS_threshold_gap_db, AS_double_range, AS_fast_detection, AS_normal_detection, AS_sample_rate, AS_input_nframes, AS_output_nframes, AS_nchannels, AS_input_duration_sec, AS_output_duration_sec, AS_actual_output_ratio","single‐value
",AudioStretchy,https://github.com/twardoch/audiostretchy,Time stretching
ALBERT can replace this; Row 17,Multimoal (Audio + Visual),Audio-Visual Analysis,"av_embeddings, transcription, score (optional)","av_embeddings: Audio-visual feature vectors for each frame, shape [num_frames, feature_dim] (e.g., 1024). transcription: Recognized text (if model is used for ASR/lip reading). score: Confidence/log probability (optional, from decoding)",AV HuBERT,https://github.com/facebookresearch/av_hubert,audio-visual speech
Emotional Recognition from audio and vision,Multimoal (Audio + Visual),Emotion recognition during the social interactions,"MELD_modality, MELD_unique_words, MELD_avg_utterance_length, MELD_max_utterance_length, MELD_avg_num_emotions_per_dialogue, MELD_num_dialogues, MELD_num_utterances, MELD_num_speakers, MELD_num_emotion_shift, MELD_avg_utterance_duration, MELD_count_anger, MELD_count_disgust, MELD_count_fear, MELD_count_joy, MELD_count_neutral, MELD_count_sadness, MELD_count_surprise

",Single values as output.,MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversation,https://github.com/declare-lab/MELD,Coversations and emotion detection
Body estimation (pose estimation),Video,Predict body-part-guided attention masks. Or the hidden posture,"PARE_pred_cam, PARE_orig_cam, PARE_verts, PARE_pose, PARE_betas, PARE_joints3d, PARE_joints2d, PARE_smpl_joints2d, PARE_bboxes, PARE_frame_ids","I’m not sure those are the final output names. If not, you can use the following names instead:
PARE_...",PARE: Part attention regressor for 3D human body estimation,https://pare.is.tue.mpg.de/,Body Pose
Body estimation (pose estimation); more simple than PARE (Row 23),Video,Pose estimation,"vit_AR, vit_AP, vit_AU, vit_mean",Single values as output.,ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation,https://github.com/ViTAE-Transformer/ViTPose,Body Pose
,Video,Estimating keypoint heatmaps and segmentation masks,"psa_AP, psa_val_mloU",,Polarized Self-Attention (PSA),https://github.com/DeLightCMU/PSA,Body Pose
Multiple people's pose estimation,Video,Keypoint localization,"rsn_gflops, rsn_ap, rsn_ap50, rsn_ap75, rsn_apm, rsn_apl, rsn_ar_head, rsn_shoulder, rsn_elbow, rsn_wrist, rsn_hip, rsn_knee, rsn_ankle, rsn_mean",,Residual Steps Network (RSN),https://github.com/caiyuanhao1998/RSN/,Body Pose
AU relation graphs,Video,"Facial action, AU relation graph","(1) BP4D: ann_AU1_bp4d, ann_AU2_bp4d, ann_AU4_bp4d, ann_AU6_bp4d, ann_AU7_bp4d, ann_AU10_bp4d, ann_AU12_bp4d, ann_AU14_bp4d, ann_AU15_bp4d, ann_AU17_bp4d, ann_AU23_bp4d, ann_AU24_bp4d, ann_avg_bp4d

(2) DISFA: ann_AU1_dis, ann_AU2_dis, ann_AU4_dis, ann_AU6_dis, ann_AU9_dis, ann_AU12_dis, ann_AU25_dis, ann_AU26_dis, ann_avg_dis

",,Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial Action Unit Recognition,https://github.com/CVI-SZU/ME-GraphAU,Facial Expression
Multi-head Cross Attention Network for Facial Expression Recognition,Video,Emotional expression indices,"dan_...; dan_emotion_scores (array with values for: anger, disgust, fear, happiness, neutral, sadness, surprise, contempt*)","the output can be dan_...; Emotion classification scores/probabilities for facial expressions. *8-class model includes contempt, 7-class excludes it. The model can also generate attention visualizations through Grad-CAM++ for explainability.",DAN: Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition,https://github.com/yaoing/DAN,Facial Expression
Temporal Modeling + action segmentation,Video,Bidirectional information transfer between action/frame features,"frame_action_labels (sequence of class indices, e.g., 0,1,2...), (optional) frame_action_probs (per-frame class probabilities) (optional","For each frame, outputs an action class label (integer). Probabilities can be extracted if needed. Labels depend on the dataset (e.g., “pour”, “crack egg”, etc.).",FACT: Frame-Action Cross-Attention Temporal Modeling for Efficient Action Segmentation,https://github.com/ZijiaLewisLu/CVPR2024-FACT,Social Actions
"Predication of facial expression, AU, valence, arousal",Video (real time),"(1) Real time video (emotional expression analytic/naming which type of emotion such as sad, happiness, etc);

(2) The prediction of valence and arousal and detection of action unit points.

","Such as: eln_arousal, eln_valence, eln_AU1, eln_AU2, eln_AU4, eln_AU6, eln_AU7, eln_AU10, eln_AU12, eln_AU15, eln_AU23, eln_AU24, eln_AU25, eln_AU26, eln_neutral_f1, eln_anger_f1, eln_disgust_f1, eln_fear_f1, eln_happiness_f1, eln_sadness_f1, eln_surprise_f1, eln_other_f1","The GitHub page doesn’t provide much information about the output names. However, according to the publication, the outputs correspond to the indices listed under “Feature Columns” in the output. You can also modify some indices using the `eln_…` naming format.","Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices",https://github.com/sb-ai-lab/EmotiEffLib/tree/main/models/affectnet_emotions,Facial Expression
Body Pose (too general and details can be checked in the webpage).,Video/Image,pose estimation and tracking,"(1) Landmarks: (single value)
GMP_land_x_1 ... GMP_land_x_33
GMP_land_y_1 ... GMP_land_y_33
GMP_land_z_1 ... GMP_land_z_33
GMP_land_visi_1 ... GMP_land_visi_33
GMP_land_presence_1 ... GMP_land_presence_33

(2) WorldLandmarks: (single value)
GMP_world_x_1 ... GMP_world_x_33
GMP_world_y_1 ... GMP_world_y_33
GMP_world_z_1 ... GMP_world_z_33
GMP_world_visi_1 ... GMP_world_visi_33
GMP_world_presence_1 ... GMP_world_presence_33

(3) SegmentationMasks: This generate the picture, the picture name is ""GMP_SM_pic"".","(1) + (2) Single value

(3) picture generation

The example output can be checked here near the bottom of webpage: https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker/python",Google MediaPipe,https://ai.google.dev/edge/mediapipe/solutions/guide,Body Pose
Pose estimation (higher resolution),Video/Image,pose estimation/precise position,"(1) Results on MPII val: 
DHiR_Head, DHiR_Shoulder, DHiR_Elbow, DHiR_Wrist, DHiR_Hip, DHiR_Knee, DHiR_Ankle, DHiR_Mean, DHiR_Meanat0.1

(2) Results on COCO val2017 with detector having human AP of 56.4 on COCO val2017 dataset
DHiR_AP, DHiR_AP_5, DHiR_AP_75, DHiR_AP_M, DHiR_AP_L, DHiR_AR, DHiR_AR_5, DHiR_AR_75, DHiR_AR_M, DHiR_AR_L",,Deep High-Resolution Representation Learning for Human Pose Estimation,https://github.com/leoxiaobin/deep-high-resolution-net.pytorch,Body Pose
Pose estimation (higher resolution),Video/Image,pose estimation and tracking,"SBH_Head, SBH_Shoulder, SBH_Elbow, SBH_Wrist, SBH_Hip, SBH_Knee, SBH_Ankle, SBH_Mean, SBH_Meanat0.1 

SBH_AP, SBH_AP_5, SBH_AP_75, SBH_AP_M, SBH_AP_L, SBH_AR, SBH_AR_5, SBH_AR_75, SBH_AR_M, SBH_AR_L",,Simple Baselines for Human Pose Estimation and Tracking,https://github.com/Microsoft/human-pose-estimation.pytorch,Body Pose
"face location in 3 dimensions, action unit annotation, emotions",Video/Image,"Actional annotation, Emotion indices, Face location and angles","pf_au01, pf_au02, pf_au04, pf_au05, pf_au06, pf_au07, pf_au09, pf_au10, pf_au11, pf_au12, pf_au14, pf_au15, pf_au17, pf_au20, pf_au23, pf_au24, pf_au25, pf_au26, pf_au28, pf_au43, pf_anger, pf_disgust, pf_fear, pf_happiness, pf_sadness, pf_surprise, pf_neutral, pf_facerectx, pf_facerecty, pf_facerectwidth, pf_facerectheight, pf_facescore, pf_pitch, pf_roll, pf_yaw, pf_x, pf_y, pf_z",,Py-Feat,Py-Feat: Python Facial Expression Analysis Toolbox — Py-Feat,Facial Expression
"Single image, action unit",Video/Image,Using continuous manifold the anatomical facial movements defining a human expression from single image,"GAN_AU*_0, GAN_AU*_33, GAN_AU*_66, GAN_AU*_99

AU* = AU1, AU2 etc that the number has bee applied. 
","It seems that the pipelines aims to generate the figures out among AUs (Action Units: AU1, AU2, AU4, AU5, AU10, AU12, AU15, AU25, AU45
)",GANimation,https://github.com/albertpumarola/GANimation,Facial Expression
label emotion on faces,Video/Image,extract the emotional indices; utilizing by different levels of features (such as low/high and multi-levels).,"arbex_primary, arbex_final","Output format is labeling of emotions: 
Neutral, Anger, Disgust, Fear, Happiness, Sadness, Surprise, Others",ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning,https://github.com/takihasan/ARBEx,Facial Expression
Pose estimation,Video/Image,pose estimation and tracking,openPose_...,"It seems that pose estimation output are figures, GIFs or the short video",Open Pose,https://github.com/CMU-Perceptual-Computing-Lab/openpose,Body Pose
Pose/Motion estimation,Video/Visual,"Dense Motion Estimation, Depth in dynamic scene, [such as the interactions between individuals and position in the video and their movement patterns in the video]","indm_abs_rel, indm_sq_rel, indm_rmse, indm_rmse_log, indm_acc_1, indm_acc_2, indm_acc_3",,Insta-DM,https://github.com/SeokjuLee/Insta-DM,-
Frame extraction tool. capture the key frame of the videos.,Video/Visual,Video frame extraction (capture the key frame of the videos),"frame_<timestamp>.jpg, extracted_frames.pdf (optional), LLM analysis JSON (optional)","Extracted key frames from the video. Filenames are in the format frame_<timestamp>.jpg, where <timestamp> is the time in seconds (e.g., frame_9.88.jpg). The tool can also perform LLM-powered analysis (using GPT-4 Vision) on the extracted frames, generating detailed JSON reports (e.g., frame_analysis.json) with key concepts, quality scores, and missing content detection. PDF export (extracted_frames.pdf) contains all frames for review.",Video Frame Extractor,https://github.com/BSM0oo/intelligent-video-frame-extractor,capture key frames from videos
Pose/Motion estimation (picture format),Video/Visual,movement and the estimation of motion,"(default: none saved; can add code) sparse_flow_vis_.png, sparse_points.npy, dense_flow.npy, dense_flow_vis_.png","Sparse: Tracked feature points and their motion (visualized as arrows). Dense: Per-pixel motion vectors (dx, dy) and color-coded flow images. By default, outputs are displayed, not saved; saving requires adding code.",Optical Flow,https://github.com/chuanenlin/optical-flow,motion estimation model
"Inviduals trajectories, locate the individuals and crowds.",Video/Visual,"Optical flow fields, Person trajectories (up to 1451), Dense pixel trajectories","(1) Common optical flow metrics (short-term): 
of_fg_static_epe_st, of_fg_static_r2_st, of_bg_static_epe_st, of_bg_static_r2_st, of_fg_dynamic_epe_st, of_fg_dynamic_r2_st, of_bg_dynamic_epe_st, of_bg_dynamic_r2_st, of_fg_avg_epe_st, of_fg_avg_r2_st, of_bg_avg_epe_st, of_bg_avg_r2_st, of_avg_epe_st, of_avg_r2_st, of_time_length_st

(2) Tracking Accuracy (long-term): 
of_ta_IM01, of_ta_IM01_Dyn, of_ta_IM02, of_ta_IM02_Dyn, of_ta_IM03, of_ta_IM03_Dyn, of_ta_IM04, of_ta_IM04_Dyn, of_ta_IM05, of_ta_IM05_Dyn, of_ta_average

(3) Person Trajectories (long-term): 
of_pt_IM01, of_pt_IM01_Dyn, of_pt_IM02, of_pt_IM02_Dyn, of_pt_IM03, of_pt_IM03_Dyn, of_pt_IM04, of_pt_IM04_Dyn, of_pt_IM05, of_pt_IM05_Dyn, of_pt_average

","You can find the result format here: https://github.com/tsenst/CrowdFlow. “st” stands for short-term, “ta” for tracking accuracy (long-term), and “pt” for person trajectories (long-term).",Optical Flow Dataset and Benchmark for Visual Crowd Analysis,https://github.com/tsenst/CrowdFlow,Optical Flow
Locate the people and object,Video/Visual,Locate the objects and people,"
ViF_consistency_1, ViF_match_1
ViF_consistency_2, ViF_match_2
ViF_consistency_3, ViF_match_3...","We can directly ask the video, “Do you see a man wearing a black T-shirt and glasses in the crowd?” (Assuming the video is three seconds long.)
The results could be:

* ViF\_consistency\_1: 1/10, ViF\_match\_1: No
* ViF\_consistency\_2: 3/10, ViF\_match\_2: No
* ViF\_consistency\_3: 9/10, ViF\_match\_3: Yes
",VideoFinder,https://github.com/win4r/VideoFinder-Llama3.2-vision-Ollama,Video Analysis
Pose estimation,Video/Visual,Pose estimation,"(1) 3D Keypoint Results: 
net_3d_estimator, net_3d_MPJPE_input_ad, net_3d_MPJPE_output_ad, net_3d_Accel_input_ad, net_3d_Accel_output_ad
(2) 2D Keypoint Results:
net_2d_estimator, net_2d_MPJPE_input_ad, net_2d_MPJPE_output_ad, net_2d_Accel_input_ad, net_2d_Accel_output_ad
(3) SMPL Results: 
net_SMPL_estimator, net_SMPL_MPJPE_input_ad, net_SMPL_MPJPE_output_ad, net_SMPL_Accel_input_ad, net_SMPL_Accel_output_ad","It seems that the outputs are not time-frame data. 
Single value/evaluation.",SmoothNet,https://github.com/cure-lab/SmoothNet,"Human motions, pose estimators"
Real-Time Intermediate Flow Estimation for Video Frame Interpolation,Video/Visual,Real-Time Intermediate Flow Estimation,"video_2X_xxfps.mp4(optional), interp__.png(optional), output/imgN.png(optional)","Interpolated video (higher frame rate) and/or interpolated frames as images. Used to create smoother motion, improve downstream feature extraction (e.g., pose, optical flow), and enable finer-grained behavioral analysis.",RIFE (Real Time Intermediate Flow Estimation),https://github.com/hzwer/ECCV2022-RIFE,Motion Estimation
Pose estimation （static）,Video/Visual,Autonomous Driving,"(1) K = 1 
GCN_min_ade_k1, GCN_min_fde_k1, GCN_MR_k1 

(2) K = 6
GCN_min_ade_k6, GCN_min_fde_k6, GCN_MR_k6",It contains both qualitative and quantitative results. The “feature columns” in the output present only the quantitative results.,LaneGCN: Learning Lane Graph Representations for Motion Forecasting,https://github.com/uber-research/LaneGCN,motion forecasting model
,Visual (language),compute dense vector representations for images,sentence_embedding_.npy or tensor(optional),"For each input sentence, outputs a fixed-length embedding vector (e.g., 768-dim). Used for semantic similarity, clustering, or as features for downstream models.",Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,https://github.com/UKPLab/sentence-transformers,Text
